{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader # loads data in batches\n",
        "from torchvision import datasets # load MNIST\n",
        "import torchvision.transforms as T # transformers for computer vision"
      ],
      "metadata": {
        "id": "q0WkjG8m15B1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm # progress bar"
      ],
      "metadata": {
        "id": "XgsRZa_a16dY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mytransform = T.ToTensor() # image (3D array) to Tensor\n",
        "\n",
        "train_data = datasets.MNIST(root = './', download=True, train = True, transform = mytransform)\n",
        "test_data = datasets.MNIST(root = './', download=True, train = False, transform = mytransform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQR7U05P2D5t",
        "outputId": "64c23820-8b6a-4898-f976-db789311fb2d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.93MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 130kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.23MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.74MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(101)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = 100, shuffle=True)\n",
        "# the test loader can be bigger and doesn't need to be shuffled\n",
        "test_loader =  DataLoader(test_data,  batch_size = 500, shuffle=False)"
      ],
      "metadata": {
        "id": "5-6HpwdJ1wDO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Loader:\n",
        "    \"\"\"\n",
        "    Custom class to accomodate train and test loader iterators\n",
        "    \"\"\"\n",
        "    train: DataLoader  # train DataLoader iterable object\n",
        "    test:  DataLoader  # test DataLoader iterable object\n",
        "\n",
        "myloader = Loader(train = train_loader, test = test_loader)\n",
        "print(myloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CXblgks1oL6",
        "outputId": "5f58acc6-2424-4c4c-e4a4-047ff1c2120c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader(train=<torch.utils.data.dataloader.DataLoader object at 0x7dbed7db4f20>, test=<torch.utils.data.dataloader.DataLoader object at 0x7dbed7db5d00>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple multilayer perceptron network\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features = 784, out_features=10):\n",
        "        \"\"\"\n",
        "        * 784 input layers\n",
        "        * 2 hiden layers of 120 and 84 neurons respectively\n",
        "        * 10 output layer\n",
        "        \"\"\"\n",
        "        super(MultilayerPerceptron, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, out_features)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        X = F.relu(self.fc1(X)) # Rectified Linear Unit (ReLU)\n",
        "        X = F.relu(self.fc2(X)) # Rectified Linear Unit (ReLU)\n",
        "        X = self.fc3(X)\n",
        "\n",
        "        return F.log_softmax(X, dim = 1) # multi-class classification, the sum of all probabilities is 1"
      ],
      "metadata": {
        "id": "e_kvP8kGwJ0k"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(101)\n",
        "\n",
        "mymodel = MultilayerPerceptron() # default params are in_features = 784, out_features=10\n",
        "mymodel # check topology"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYx3h80_1MWY",
        "outputId": "ae54cacc-9c9b-4a2f-c1a8-d640432924b9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultilayerPerceptron(\n",
              "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(mymodel.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "rV_E6NsU1SaV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [p.numel() for p in mymodel.parameters() if p.requires_grad]\n",
        "np.sum(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0T3CRR21Tzz",
        "outputId": "2b8b0db9-d42f-47a2-d227-ded40947d1ce"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(105214)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 10 images\n",
        "myiter = iter(myloader.train)\n",
        "img, label = myiter.__next__() # only one iteration\n",
        "img.shape # batch_size, channel, Height, Width"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQNZFhxZ1VaQ",
        "outputId": "e8c0b95d-ed18-47a7-8f9d-ac3132b3b979"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = mymodel( img.view(100,-1) )\n",
        "y_pred.shape # 100 x 10, meaning for every batch (100) we obtain  (10 probabilities) predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bt-tPlg1X3d",
        "outputId": "6f614858-2436-4385-dd93-48f54f94adc6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val, idx = torch.max(y_pred, dim=1) # dim 1 is for the output\n",
        "idx # indices == predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAVUC-Cu1ZLm",
        "outputId": "14ca581b-6986-4f6e-ffd6-665cd37401f2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 2, 2, 2, 8, 2, 2, 8, 2, 6, 8, 8, 8, 8, 8, 8, 8, 2, 6, 2, 8, 2, 2, 2,\n",
              "        8, 2, 6, 8, 6, 2, 6, 8, 2, 2, 8, 7, 6, 6, 2, 2, 2, 8, 2, 2, 6, 6, 8, 8,\n",
              "        8, 2, 2, 7, 8, 6, 2, 8, 2, 2, 2, 6, 2, 2, 2, 6, 7, 2, 8, 8, 6, 6, 2, 8,\n",
              "        2, 6, 7, 8, 6, 8, 6, 2, 2, 2, 2, 2, 2, 6, 8, 8, 2, 8, 6, 2, 2, 6, 2, 2,\n",
              "        8, 8, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tracking variables\n",
        "\n",
        "class Loss:\n",
        "    \"\"\" Class to monitor train and test lost\"\"\"\n",
        "    train: list = []\n",
        "    test: list = []\n",
        "\n",
        "\n",
        "class Accuracy:\n",
        "    \"\"\" Class to monitor train and test accuracy\"\"\"\n",
        "    train: list = []\n",
        "    test: list = []"
      ],
      "metadata": {
        "id": "P3xggfvj1acK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "epochs = 10\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    # we set the number of True positives to zero in every epoch\n",
        "    train_corr = 0\n",
        "    test_corr = 0\n",
        "\n",
        "    # training: we run the train dataset in batches of 100 images, meaning that run the loader 600 times (600 x 100 = 60,000 images)\n",
        "    for batch, (img, label) in enumerate(myloader.train):\n",
        "        batch +=1\n",
        "        y_pred = mymodel( img.view(100,-1) ) # batch size for train is 100\n",
        "        loss = criterion(y_pred, label)\n",
        "\n",
        "        # last 10-layer neurons into one result\n",
        "        _, prediction = torch.max(y_pred,dim = 1)\n",
        "        train_corr += (prediction == label).sum() # sum of correct predictions\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # monitor loss and accuracy every 200 train batches of 100 images ( 3 times per epoch)\n",
        "        if batch%200 == 0:\n",
        "            acc = 100 * (train_corr.item() / (batch*100) ) # correct predictions divided by images in batches\n",
        "            print( f'Epoch:{epoch:2d} batch: {batch:2d} loss: {loss.item():4.4f} Accuracy: {acc:4.4f} %' )\n",
        "\n",
        "    Loss.train.append( loss.item() ) # store loss at the end of epoch\n",
        "    accuracy = 100 * (train_corr.item() / (batch*100) ) # accuracy is 100 x train_corr.item()/60_000\n",
        "    Accuracy.train.append( accuracy ) # store accuracy at the end of epoch\n",
        "\n",
        "\n",
        "    # validation (test). Here we run batches of 500 images, the test loader runs 120 times (120 x 500 = 60,000)\n",
        "    with torch.no_grad():\n",
        "        for batch, (img, label) in enumerate(myloader.test):\n",
        "            batch +=1\n",
        "            y_val = mymodel( img.view(500,-1) ) # batch size for test is 500\n",
        "            _, predicted = torch.max( y_val, dim = 1)\n",
        "            test_corr += (predicted == label).sum()\n",
        "\n",
        "    loss = criterion( y_val,label )\n",
        "    Loss.test.append( loss.item() )\n",
        "    accuracy = 100 * (test_corr.item()/ (batch*500) ) # accuracy is 100 x train_corr.item()/60,000\n",
        "    Accuracy.test.append( accuracy )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgd1YmZC1a98",
        "outputId": "73addf20-7e00-4b25-babc-4a63f5cb22c2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 batch: 200 loss: 0.2996 Accuracy: 82.6550 %\n",
            "Epoch: 0 batch: 400 loss: 0.2726 Accuracy: 87.2475 %\n",
            "Epoch: 0 batch: 600 loss: 0.1815 Accuracy: 89.3183 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:11<01:40, 11.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 batch: 200 loss: 0.1704 Accuracy: 94.6100 %\n",
            "Epoch: 1 batch: 400 loss: 0.0507 Accuracy: 95.0100 %\n",
            "Epoch: 1 batch: 600 loss: 0.1428 Accuracy: 95.2500 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:22<01:28, 11.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 batch: 200 loss: 0.1678 Accuracy: 96.5200 %\n",
            "Epoch: 2 batch: 400 loss: 0.1715 Accuracy: 96.5000 %\n",
            "Epoch: 2 batch: 600 loss: 0.1084 Accuracy: 96.5800 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:33<01:18, 11.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 batch: 200 loss: 0.1408 Accuracy: 97.4350 %\n",
            "Epoch: 3 batch: 400 loss: 0.0127 Accuracy: 97.4800 %\n",
            "Epoch: 3 batch: 600 loss: 0.0746 Accuracy: 97.4133 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:44<01:07, 11.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 batch: 200 loss: 0.0551 Accuracy: 98.0250 %\n",
            "Epoch: 4 batch: 400 loss: 0.1177 Accuracy: 97.9375 %\n",
            "Epoch: 4 batch: 600 loss: 0.1012 Accuracy: 97.9800 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:56<00:56, 11.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 batch: 200 loss: 0.0772 Accuracy: 98.5200 %\n",
            "Epoch: 5 batch: 400 loss: 0.0259 Accuracy: 98.4225 %\n",
            "Epoch: 5 batch: 600 loss: 0.0323 Accuracy: 98.3583 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:07<00:45, 11.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6 batch: 200 loss: 0.0233 Accuracy: 98.8550 %\n",
            "Epoch: 6 batch: 400 loss: 0.0402 Accuracy: 98.6800 %\n",
            "Epoch: 6 batch: 600 loss: 0.0302 Accuracy: 98.6633 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [01:19<00:34, 11.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7 batch: 200 loss: 0.0042 Accuracy: 99.0150 %\n",
            "Epoch: 7 batch: 400 loss: 0.0070 Accuracy: 98.9075 %\n",
            "Epoch: 7 batch: 600 loss: 0.0336 Accuracy: 98.8567 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:29<00:22, 11.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8 batch: 200 loss: 0.0085 Accuracy: 99.1950 %\n",
            "Epoch: 8 batch: 400 loss: 0.0787 Accuracy: 99.1750 %\n",
            "Epoch: 8 batch: 600 loss: 0.0494 Accuracy: 99.1383 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:40<00:11, 11.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9 batch: 200 loss: 0.0469 Accuracy: 99.3500 %\n",
            "Epoch: 9 batch: 400 loss: 0.0213 Accuracy: 99.3225 %\n",
            "Epoch: 9 batch: 600 loss: 0.0210 Accuracy: 99.2500 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:51<00:00, 11.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 50s, sys: 153 ms, total: 1min 50s\n",
            "Wall time: 1min 51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys"
      ],
      "metadata": {
        "id": "yCmzzeD22TQC"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageOps\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def preprocess_image(image_path, show_processed_image=False):\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('L')\n",
        "    except IOError:\n",
        "        print(f\"Hata: '{image_path}' yolu açılamadı.\")\n",
        "        return None, None\n",
        "\n",
        "    img_inverted = ImageOps.invert(img)\n",
        "\n",
        "    img_resized = img_inverted.resize((28, 28), Image.LANCZOS)\n",
        "\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    tensor = preprocess(img_resized)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "\n",
        "    if show_processed_image:\n",
        "        plt.imshow(img_resized, cmap='gray')\n",
        "        plt.axis('off') # Eksenleri gizle\n",
        "        plt.show()\n",
        "\n",
        "    return tensor\n",
        "\n",
        "IMAGE_PATH = \"digit_5.jpg\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Kullanılan cihaz: {device}\")\n",
        "\n",
        "mymodel.eval()\n",
        "\n",
        "# 2. Görüntüyü işle\n",
        "image_tensor = preprocess_image(IMAGE_PATH, show_processed_image=True)\n",
        "\n",
        "if image_tensor is not None:\n",
        "    image_tensor = image_tensor.to(device)\n",
        "\n",
        "    # 3. Tahmin\n",
        "    with torch.no_grad():\n",
        "        output = mymodel(image_tensor.view(image_tensor.shape[0], -1))\n",
        "        prediction = torch.argmax(output, dim=1)\n",
        "\n",
        "        probabilities = F.softmax(output, dim=1)\n",
        "        top_prob, top_class = probabilities.topk(1, dim=1)\n",
        "\n",
        "        print(\"---\")\n",
        "        print(f\"Tahmin Edilen Rakam: {prediction.item()}\")\n",
        "        print(f\"Modelin Emin Olma Olasılığı: {top_prob.item()*100:.2f}%\")\n",
        "        print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "7PGdmgqhkMhL",
        "outputId": "fe6b7604-b23b-4087-a931-1633f454aaae"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullanılan cihaz: cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "Tahmin Edilen Rakam: 1\n",
            "Modelin Emin Olma Olasılığı: 46.77%\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSFFzOGrnkDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}